{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Enter the text : \") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:  ['rahul', 'should', 'clean', 'his', 'room', 'by', '5', 'pm', 'today']\n",
      "POS Tags:  ['rahul(PROPN)', 'should(AUX)', 'clean(VERB)', 'his(PRON)', 'room(NOUN)', 'by(ADP)', '5(NUM)', 'pm(NOUN)', 'today(NOUN)']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    cleaned_text = \" \"\n",
    "    pos_tags_str = \" \"\n",
    "    \n",
    "    # Process text using spaCy\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        # Remove stop words and punctuation\n",
    "        cleaned_tokens = [token.text for token in doc if token.text not in string.punctuation]\n",
    "    \n",
    "        # Perform POS tagging\n",
    "        pos_tags = [f\"{token.text}({token.pos_})\" for token in doc if token.text not in string.punctuation]\n",
    "    \n",
    "        # Convert lists to strings\n",
    "        cleaned_text += str(cleaned_tokens)\n",
    "        pos_tags_str += str(pos_tags)\n",
    "    \n",
    "    return cleaned_text, pos_tags_str\n",
    "\n",
    "# Preprocess text\n",
    "cleaned_text, pos_tags_str = preprocess_text(text)\n",
    "\n",
    "# Print results\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "print(\"POS Tags:\", pos_tags_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Word: Rahul, POS Tag: PROPN\n",
      "Word: should, POS Tag: AUX\n",
      "Word: clean, POS Tag: VERB\n",
      "Word: his, POS Tag: PRON\n",
      "Word: room, POS Tag: NOUN\n",
      "Word: by, POS Tag: ADP\n",
      "Word: 5, POS Tag: NUM\n",
      "Word: pm, POS Tag: NOUN\n",
      "Word: today, POS Tag: NOUN\n",
      "Word: ., POS Tag: PUNCT\n"
     ]
    }
   ],
   "source": [
    "from spacy import tokens\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "print(type(doc))\n",
    "# Tokenization and POS tagging\n",
    "for token in doc:\n",
    "    print(f\"Word: {token}, POS Tag: {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and identifying tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['rahul', 'should', 'clean', 'his', 'room', 'by', '5', 'pm', 'today']\n",
      "Identified Tasks: [\" ['rahul', 'should', 'clean', 'his', 'room', 'by', '5', 'pm', 'today']\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def identify_tasks(sentences):\n",
    "    tasks = []\n",
    "    modal_verbs = {\"should\", \"must\", \"have to\", \"needs to\", \"will\", \"can\", \"could\", \"may\", \"might\", \"ought to\"}\n",
    "    sentences = sent_tokenize(sentences)\n",
    "    for sentence in sentences:\n",
    "        # Tokenize and process each sentence\n",
    "        doc = nlp(sentence)\n",
    "        print(doc)\n",
    "        verbs = []\n",
    "        \n",
    "        # Manually check for verb phrases (e.g., \"needs to\", \"have to\")\n",
    "        words = sentence.lower().split()\n",
    "        \n",
    "        # Check for specific two-word phrases like \"needs to\", \"have to\"\n",
    "        for i in range(len(words) - 1):\n",
    "            if words[i] == \"needs\" and words[i+1] == \"to\":\n",
    "                verbs.append(\"needs to\")\n",
    "            if words[i] == \"have\" and words[i+1] == \"to\":\n",
    "                verbs.append(\"have to\")\n",
    "        \n",
    "        # If no verb phrase was found, check for regular verbs in the sentence\n",
    "        if not verbs:\n",
    "            verbs = [token for token in doc if token.pos_ == \"VERB\" or token.text.lower() in modal_verbs]\n",
    "        \n",
    "        # If we found verbs or modal verbs, assume this sentence contains a task\n",
    "        if verbs:\n",
    "            tasks.append(sentence)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "tasks = identify_tasks(cleaned_text)\n",
    "print(\"Identified Tasks:\", tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.111*\"\\'5\\',\" + 0.111*\"\\'today\\']\" + 0.111*\"\\'clean\\',\" + 0.111*\"[\\'rahul\\',\"')\n",
      "(1, '0.111*\"\\'by\\',\" + 0.111*\"\\'5\\',\" + 0.111*\"\\'room\\',\" + 0.111*\"\\'today\\']\"')\n",
      "(2, '0.111*\"\\'today\\']\" + 0.111*\"\\'by\\',\" + 0.111*\"\\'pm\\',\" + 0.111*\"[\\'rahul\\',\"')\n",
      "(3, '0.111*\"\\'his\\',\" + 0.111*\"\\'should\\',\" + 0.111*\"\\'room\\',\" + 0.111*\"\\'pm\\',\"')\n",
      "(4, '0.111*\"\\'today\\']\" + 0.111*\"[\\'rahul\\',\" + 0.111*\"\\'clean\\',\" + 0.111*\"\\'by\\',\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Tokenize and create a dictionary and corpus\n",
    "texts = [[word for word in doc.lower().split()] for doc in tasks]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Extract names and deadlines\n",
    "def extract_task_info(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract responsible person's name (Proper Nouns)\n",
    "    names = [ent.text for ent in doc if ent.ent_type_ == \"PERSON\"]\n",
    "    responsible_person = names[0] if names else \"Not specified\"\n",
    "    \n",
    "    # Extract deadline (words like \"tomorrow\", \"by Friday\", etc.)\n",
    "    deadline_patterns = r'\\b(tomorrow|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|next week|next month|by \\w+ \\d*|at \\d+ (AM|PM))\\b'\n",
    "    deadline_match = re.search(deadline_patterns, text, re.IGNORECASE)\n",
    "    deadline = deadline_match.group() if deadline_match else \"No deadline\"\n",
    "\n",
    "    return responsible_person, deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  ['rahul', 'should', 'clean', 'his', 'room', 'by', '5', 'pm', 'today']\n",
      "Category: Uncategorized\n",
      "Assigned To: Not specified\n",
      "Deadline: No deadline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assign categories to tasks\n",
    "def categorize_task(task_text):\n",
    "    bow_vector = dictionary.doc2bow(task_text.lower().split())\n",
    "    topic_distribution = lda[bow_vector]\n",
    "    \n",
    "    # Get the highest probability topic\n",
    "    topic_num = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    \n",
    "    category_mapping = {\n",
    "        0: \"Meetings & Reviews\",\n",
    "        1: \"Housekeeping & Maintenance\",\n",
    "        2: \"Project & Client Work\"\n",
    "    }\n",
    "    \n",
    "    return category_mapping.get(topic_num, \"Uncategorized\")\n",
    "\n",
    "# Generate structured task list\n",
    "structured_tasks = []\n",
    "\n",
    "for task in tasks:\n",
    "    responsible_person, deadline = extract_task_info(task)\n",
    "    category = categorize_task(task)\n",
    "    structured_tasks.append({\n",
    "        \"Task\": task,\n",
    "        \"Category\": category,\n",
    "        \"Assigned To\": responsible_person,\n",
    "        \"Deadline\": deadline\n",
    "    })\n",
    "\n",
    "# Print structured task list\n",
    "for task_info in structured_tasks:\n",
    "    print(f\"Task: {task_info['Task']}\")\n",
    "    print(f\"Category: {task_info['Category']}\")\n",
    "    print(f\"Assigned To: {task_info['Assigned To']}\")\n",
    "    print(f\"Deadline: {task_info['Deadline']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
